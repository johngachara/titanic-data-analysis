{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDvKQXjS86tMCED8Yaz8tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johngachara/titanic-data-analysis/blob/main/DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNS3UQEYyKmB"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = sns.load_dataset('titanic')\n",
        "#check dataset structure\n",
        "print(titanic.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbzhfDHmyeBV",
        "outputId": "b825eb38-97cc-4994-fd1f-278cd90a412e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 15 columns):\n",
            " #   Column       Non-Null Count  Dtype   \n",
            "---  ------       --------------  -----   \n",
            " 0   survived     891 non-null    int64   \n",
            " 1   pclass       891 non-null    int64   \n",
            " 2   sex          891 non-null    object  \n",
            " 3   age          714 non-null    float64 \n",
            " 4   sibsp        891 non-null    int64   \n",
            " 5   parch        891 non-null    int64   \n",
            " 6   fare         891 non-null    float64 \n",
            " 7   embarked     889 non-null    object  \n",
            " 8   class        891 non-null    category\n",
            " 9   who          891 non-null    object  \n",
            " 10  adult_male   891 non-null    bool    \n",
            " 11  deck         203 non-null    category\n",
            " 12  embark_town  889 non-null    object  \n",
            " 13  alive        891 non-null    object  \n",
            " 14  alone        891 non-null    bool    \n",
            "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
            "memory usage: 80.7+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4id662cBzUpa",
        "outputId": "dadf9160-7fd3-44d0-b3d6-6cacbfda7300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         survived      pclass         age       sibsp       parch        fare\n",
            "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
            "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
            "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
            "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
            "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
            "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
            "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
            "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing = titanic.isnull().sum()\n",
        "print(missing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juEDpd5W0Vl4",
        "outputId": "7d607ad7-5db8-4bf5-85dc-e2219f0fdd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "survived         0\n",
            "pclass           0\n",
            "sex              0\n",
            "age            177\n",
            "sibsp            0\n",
            "parch            0\n",
            "fare             0\n",
            "embarked         2\n",
            "class            0\n",
            "who              0\n",
            "adult_male       0\n",
            "deck           688\n",
            "embark_town      2\n",
            "alive            0\n",
            "alone            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic['age'].fillna(titanic['age'].median(),inplace=True)\n",
        "print(titanic.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UluDYM0sn9",
        "outputId": "c19819d0-2b12-4be1-ac04-59c26d16d3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "survived         0\n",
            "pclass           0\n",
            "sex              0\n",
            "age              0\n",
            "sibsp            0\n",
            "parch            0\n",
            "fare             0\n",
            "embarked         2\n",
            "class            0\n",
            "who              0\n",
            "adult_male       0\n",
            "deck           688\n",
            "embark_town      2\n",
            "alive            0\n",
            "alone            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.drop(columns=['deck'],inplace=True)"
      ],
      "metadata": {
        "id": "-IUm1haq7wIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "titanic['embark_town'].fillna(titanic['embark_town'].mode()[0],inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0],inplace=True)\n",
        "print(titanic.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxn2XdHG8PK8",
        "outputId": "75c3470e-c995-4e9c-a91b-be52dcc59401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "survived       0\n",
            "pclass         0\n",
            "sex            0\n",
            "age            0\n",
            "sibsp          0\n",
            "parch          0\n",
            "fare           0\n",
            "embarked       0\n",
            "class          0\n",
            "who            0\n",
            "adult_male     0\n",
            "embark_town    0\n",
            "alive          0\n",
            "alone          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#z Score outlier\n",
        "data = titanic['fare']\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data) # calculates the standard deviation\n",
        "Z_scores = (data - mean) / std_dev # computes the Z-scores\n",
        "outliers = data[np.abs(Z_scores) > 3] # finds all the data points that are 3 standard deviations away from the mean\n",
        "data2 = titanic['age']\n",
        "mean2 = data2.mean()\n",
        "std2 = data2.std()\n",
        "z_score = (data2-mean2)/std2\n",
        "outliers2 = data2[np.abs(z_score)>3]\n",
        "print(outliers2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuGFlJBj8wJV",
        "outputId": "df6dbabf-54c0-4f30-9d39-b1b490c963f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96     71.0\n",
            "116    70.5\n",
            "493    71.0\n",
            "630    80.0\n",
            "672    70.0\n",
            "745    70.0\n",
            "851    74.0\n",
            "Name: age, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = titanic['fare'].quantile(0.25) # calculates the first quartile\n",
        "Q3 = titanic['fare'].quantile(0.75) # calculates the third quartile\n",
        "IQR = Q3 - Q1 # computes the IQR\n",
        "\n",
        "# Below, we find all the data points that fall below the lower bound or above the upper bound\n",
        "outliers = titanic['fare'][\n",
        "    (titanic['fare'] < (Q1 - 1.5 * IQR)) |   #less than lower bound\n",
        "    (titanic['fare'] > (Q3 + 1.5 * IQR))     # greater than upper bound\n",
        "]\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "FglDXkNZAf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1948c034-8967-4eb2-92e6-d9b02862fb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1       71.2833\n",
            "27     263.0000\n",
            "31     146.5208\n",
            "34      82.1708\n",
            "52      76.7292\n",
            "         ...   \n",
            "846     69.5500\n",
            "849     89.1042\n",
            "856    164.8667\n",
            "863     69.5500\n",
            "879     83.1583\n",
            "Name: fare, Length: 116, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(titanic['fare']) # calculates the mean\n",
        "standard_deviation = np.std(titanic['fare']) # calculates the standard deviation\n",
        "outliers = titanic['fare'][np.abs(titanic['fare'] - mean) > 3 * standard_deviation] # finds all the data points that are 3 standard deviations away from the mean"
      ],
      "metadata": {
        "id": "IekHeEUJAmTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cappping replacing outlier values with a certain maximum or minimum\n",
        "# Drop rows with missing 'age' values\n",
        "titanic_df = titanic.dropna(subset=['age'])\n",
        "\n",
        "# Calculate the upper bound for 'age'\n",
        "Q1 = titanic_df['age'].quantile(0.25)\n",
        "Q3 = titanic_df['age'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap the outliers for 'age'\n",
        "titanic_df['age'] = np.where(titanic_df['age'] > upper_bound, upper_bound, titanic_df['age'])\n",
        "\n",
        "# Calculate the upper bound for 'fare'\n",
        "Q1 = titanic_df['fare'].quantile(0.25)\n",
        "Q3 = titanic_df['fare'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap the outliers for 'fare'\n",
        "titanic_df['fare'] = np.where(titanic_df['fare'] > upper_bound, upper_bound, titanic_df['fare'])"
      ],
      "metadata": {
        "id": "ikSO3RUKAwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalization\n",
        "\n",
        "titanic_df[['age','fare']] = MinMaxScaler(feature_range=(0,10)).fit_transform(titanic_df[['age','fare']])\n",
        "print(titanic_df[['age','fare']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxuoTdLh3cVj",
        "outputId": "26f95cba-03f8-4a50-8653-e2ea8b709d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        age       fare\n",
            "0  3.990385   1.104604\n",
            "1  6.948964  10.000000\n",
            "2  4.730030   1.207446\n",
            "3  6.394231   8.090270\n",
            "4  6.394231   1.226491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding of categorical features\n",
        "encoded = pd.get_dummies(titanic_df,columns=['sex','embarked'],dtype=int)\n",
        "print(encoded.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaymZN0i4j4W",
        "outputId": "0df0075a-535a-45d3-f9a7-17c9c2c62bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass       age  sibsp  parch       fare  class    who  \\\n",
            "0         0       3  3.990385      1      0   1.104604  Third    man   \n",
            "1         1       1  6.948964      1      0  10.000000  First  woman   \n",
            "2         1       3  4.730030      0      0   1.207446  Third  woman   \n",
            "3         1       1  6.394231      1      0   8.090270  First  woman   \n",
            "4         0       3  6.394231      0      0   1.226491  Third    man   \n",
            "\n",
            "   adult_male  embark_town alive  alone  sex_female  sex_male  embarked_C  \\\n",
            "0        True  Southampton    no  False           0         1           0   \n",
            "1       False    Cherbourg   yes  False           1         0           1   \n",
            "2       False  Southampton   yes   True           1         0           0   \n",
            "3       False  Southampton   yes  False           1         0           0   \n",
            "4        True  Southampton    no   True           0         1           0   \n",
            "\n",
            "   embarked_Q  embarked_S  \n",
            "0           0           1  \n",
            "1           0           0  \n",
            "2           0           1  \n",
            "3           0           1  \n",
            "4           0           1  \n"
          ]
        }
      ]
    },
    {
      "source": [
        "titanic_df = pd.concat([titanic_df.drop(columns=['sex','embarked']),encoded],axis=1)\n",
        "print(titanic_df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paGEz8db5XUU",
        "outputId": "98462e65-cadb-4505-9ee2-33389ddcbe3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass       age  sibsp  parch       fare  class    who  \\\n",
            "0         0       3  3.990385      1      0   1.104604  Third    man   \n",
            "1         1       1  6.948964      1      0  10.000000  First  woman   \n",
            "2         1       3  4.730030      0      0   1.207446  Third  woman   \n",
            "3         1       1  6.394231      1      0   8.090270  First  woman   \n",
            "4         0       3  6.394231      0      0   1.226491  Third    man   \n",
            "\n",
            "   adult_male  embark_town  ...    who  adult_male  embark_town  alive  alone  \\\n",
            "0        True  Southampton  ...    man        True  Southampton     no  False   \n",
            "1       False    Cherbourg  ...  woman       False    Cherbourg    yes  False   \n",
            "2       False  Southampton  ...  woman       False  Southampton    yes   True   \n",
            "3       False  Southampton  ...  woman       False  Southampton    yes  False   \n",
            "4        True  Southampton  ...    man        True  Southampton     no   True   \n",
            "\n",
            "   sex_female  sex_male  embarked_C embarked_Q embarked_S  \n",
            "0           0         1           0          0          1  \n",
            "1           1         0           1          0          0  \n",
            "2           1         0           0          0          1  \n",
            "3           1         0           0          0          1  \n",
            "4           0         1           0          0          1  \n",
            "\n",
            "[5 rows x 29 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization and normalization\n",
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Load the Titanic Dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "\n",
        "# Create a MinMaxScaler object with a feature range of 0 to 100\n",
        "age_scaler = MinMaxScaler(feature_range=(0, 100))\n",
        "# TODO: Create a StandardScaler object for standardizing 'fare'\n",
        "fare_scaler = StandardScaler()\n",
        "# Fit the age scaler on the age data without NaN values\n",
        "age_scaler.fit(titanic_df[['age']].dropna())\n",
        "# TODO: Fit the scaler on the 'fare' data without NaN values\n",
        "fare_scaler.fit(titanic_df[['fare']].dropna())\n",
        "# Holds the indexes for the rows with non-NaN age and fare values\n",
        "non_na_age_index = titanic_df['age'].dropna().index\n",
        "# TODO: Replace the following line with a calculation of the non-NaN fare index values\n",
        "non_na_fare_index = titanic_df['fare'].dropna().index\n",
        "\n",
        "# Transform the 'age' and 'fare' columns in the original dataframe without NaN values\n",
        "titanic_df.loc[non_na_age_index, 'norm_age'] = age_scaler.transform(titanic_df.loc[non_na_age_index, ['age']])\n",
        "# TODO: Transform the 'fare' column using the StandardScaler and non-NaN indices\n",
        "titanic_df.loc[non_na_fare_index, 'stand_fare'] = fare_scaler.transform(titanic_df.loc[non_na_fare_index, ['fare']])\n",
        "# Display transformed 'age' and standardized 'fare' values\n",
        "# TODO: Update this line to include the newly standardized 'fare' column\n",
        "print(titanic_df[['age', 'norm_age', 'fare','stand_fare']])"
      ],
      "metadata": {
        "id": "IbMnYQtidC3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO: Load the Titanic Dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "# TODO: Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "# TODO: Fit the scaler on the 'fare' data while handling NaN values properly\n",
        "fare = titanic_df[['fare']].dropna()\n",
        "scaler.fit(fare)\n",
        "# TODO: Transform the 'fare' column creating a new column 'stand_fare' in the original dataframe without NaN values\n",
        "fare_na_index = titanic_df['fare'].dropna().index\n",
        "titanic_df.loc[fare_na_index,'stand_fare'] = scaler.transform(titanic_df.loc[fare_na_index,['fare']])\n",
        "# TODO: Display standardized 'fare' values (the new 'stand_fare' column)\n",
        "print(titanic_df['stand_fare'])"
      ],
      "metadata": {
        "id": "tmNoWO4yghok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Create a new feature 'family_size'\n",
        "titanic_df['family_size'] = titanic_df['sibsp'] + titanic_df['parch'] + 1\n",
        "\n",
        "# Modify 'fare' feature to 'log_fare' using natural logarithm\n",
        "titanic_df['log_fare'] = np.log10(titanic_df['fare'] + 0.1)\n",
        "\n",
        "# One-hot encode 'sex' feature\n",
        "sex_dummies = pd.get_dummies(titanic_df['sex'], dtype=int)\n",
        "titanic_df = pd.concat([titanic_df, sex_dummies], axis=1)\n",
        "\n",
        "# Print the first 5 rows of the dataframe\n",
        "print(titanic_df.head())"
      ],
      "metadata": {
        "id": "4Xm7IEp-j-Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Create a new binary encoded feature 'embarked_southampton'\n",
        "embark_town_col = pd.DataFrame([1 if i == 'Southampton' else 0 for i in titanic_df['embark_town']], columns=[\"embarked_southampton\"])\n",
        "\n",
        "# Join to the main dataframe with aligned indices\n",
        "titanic_df = titanic_df.reset_index(drop=True)\n",
        "embark_town_col = embark_town_col.reset_index(drop=True)\n",
        "titanic_df = pd.concat([titanic_df, embark_town_col], axis=1)\n",
        "\n",
        "# Print the first 5 rows of the dataframe\n",
        "print(titanic_df.head())"
      ],
      "metadata": {
        "id": "Hsb27npForlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Create a new feature 'family_size'\n",
        "titanic_df['family_size'] = titanic_df['sibsp'] + titanic_df['parch'] + 1\n",
        "\n",
        "# TODO: Transform the 'fare' column into a new column 'log10_fare' using log base 10. Include an adjustment for zero fares.\n",
        "titanic_df['log10_fare'] = np.log10(titanic_df['fare'] + 0.1)\n",
        "\n",
        "# Print the first 5 rows of the dataframe\n",
        "print(titanic_df.head())"
      ],
      "metadata": {
        "id": "J1hxfj3CqDQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Load the Titanic dataset and assign it to a variable named 'titanic_df'\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "# TODO: Perform one-hot encoding on the 'class' column to create binary columns for each class\n",
        "class_col = titanic_df['class']\n",
        "binary = pd.get_dummies(class_col,columns=['class'],dtype=int)\n",
        "# TODO: Join the new binary columns to 'titanic_df'\n",
        "titanic_df = pd.concat([titanic_df,binary],axis=1)\n",
        "# TODO: Display the first 5 rows of the modified dataframe\n",
        "print(titanic_df.head())"
      ],
      "metadata": {
        "id": "hpjrZjKcrUKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# Loading the Titanic dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "\n",
        "# Splitting the full dataset into the training and testing datasets\n",
        "train_data, test_data = train_test_split(titanic_df, test_size=0.4, random_state=42)\n",
        "\n",
        "# Printing out the shapes of the datasets\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ],
      "metadata": {
        "id": "2Kwr9S3YtSwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess the Titanic dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "\n",
        "# One-hot encode categorical variables using pandas get_dummies\n",
        "titanic_preprocessed = pd.get_dummies(titanic_df, columns=['sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], drop_first=True)\n",
        "\n",
        "# Handle any NaN values by filling them with the mean of the column\n",
        "titanic_preprocessed = titanic_preprocessed.fillna(titanic_preprocessed.mean())\n",
        "\n",
        "# Split the preprocessed dataset into the training and testing datasets with a 70%-30% split\n",
        "train_data, test_data = train_test_split(titanic_preprocessed, test_size=0.3, random_state=42)\n",
        "\n",
        "# Separate the target variable (\"survived\") from the rest of the training data\n",
        "x_train = train_data.drop(\"survived\", axis=1)\n",
        "y_train = train_data[\"survived\"]\n",
        "x_test = test_data.drop(\"survived\",axis=1)\n",
        "y_test = test_data['survived']\n",
        "# Initialize a Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
        "\n",
        "# Training the Logistic Regression model\n",
        "logreg.fit(x_train, y_train)\n",
        "\n",
        "# Using the model to make predictions on the testing dataset\n",
        "predictions = logreg.predict(x_test)\n",
        "\n",
        "# Displaying metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions))  # Will produce incorrect results due to the bug\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))  # Will produce incorrect results due to the bug\n",
        "\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test, predictions))  # Will produce incorrect results due to the bug"
      ],
      "metadata": {
        "id": "utupZAqMwQ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess the Titanic dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop non-numeric columns for simplicity\n",
        "titanic_df = titanic_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Handle any NaN values by filling them with the mean of the column\n",
        "titanic_df = titanic_df.fillna(titanic_df.mean())\n",
        "\n",
        "# TODO: Use MinMaxScaler to scale the numeric features into a standard range\n",
        "# Hint: You will need to create an instance of MinMaxScaler, fit it on the data and transform the data\n",
        "scaler = MinMaxScaler()\n",
        "titanic_df_scaled = scaler.fit_transform(titanic_df)\n",
        "titanic_df = pd.DataFrame(titanic_df_scaled,columns=titanic_df.columns)\n",
        "# Split the preprocessed dataset into the training and testing datasets with a 70%-30% split\n",
        "train_data, test_data = train_test_split(titanic_df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Separate the target variable (\"survived\") from the rest of the training data\n",
        "x_train = train_data.drop(\"survived\", axis=1)\n",
        "y_train = train_data[\"survived\"]\n",
        "\n",
        "# Initialize a Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Training the Logistic Regression model\n",
        "logreg.fit(x_train, y_train)\n",
        "\n",
        "# Separate the independent (x_test) and dependent (y_test) variables from the testing dataset\n",
        "x_test = test_data.drop(\"survived\", axis=1)\n",
        "y_test = test_data[\"survived\"]\n",
        "\n",
        "# Using the model to make predictions on the testing dataset\n",
        "predictions = logreg.predict(x_test)\n",
        "\n",
        "# Displaying metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "id": "K2hWLrT8y_mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score  # Changed from precision_score to accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_df = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop columns with strings and 'pclass', which is categorical but read as a numeric type\n",
        "titanic_df = titanic_df.select_dtypes(exclude=['object', 'category'])\n",
        "\n",
        "# Handle any NaN values by filling them with the mean of the column (ignoring 'pclass', which is categorical)\n",
        "numeric_columns = titanic_df.columns.drop('pclass')\n",
        "titanic_df[numeric_columns] = titanic_df[numeric_columns].fillna(titanic_df[numeric_columns].mean())\n",
        "\n",
        "# Convert 'pclass' to integer type if it's not already\n",
        "titanic_df['pclass'] = titanic_df['pclass'].astype(int)\n",
        "\n",
        "# TODO: Split the dataset into training and testing sets with a 70%-30% split\n",
        "train_data,test_data = train_test_split(titanic_df,test_size=0.3,random_state=42)\n",
        "# TODO: Identify and separate the target variable 'survived' from the training and testing data\n",
        "x_train = train_data.drop(\"survived\",axis=1)\n",
        "y_train = train_data['survived']\n",
        "x_test = test_data.drop(\"survived\",axis=1)\n",
        "y_test = test_data['survived']\n",
        "# TODO: Initialize StandardScaler and scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)#we fit one time\n",
        "# TODO: Initialize the Logistic Regression model and train it on the scaled training data\n",
        "logreg = LogisticRegression(max_iter=100)\n",
        "logreg.fit(x_train_scaled,y_train)\n",
        "# TODO: Use the trained model to make predictions on the scaled testing data\n",
        "predictions = logreg.predict(x_test_scaled)\n",
        "# TODO: Calculate and print the accuracy score\n",
        "print(accuracy_score(y_test,predictions))"
      ],
      "metadata": {
        "id": "8DByJ-Wc230e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the dataset\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y = raw_df.values[1::2, 2]\n",
        "\n",
        "# Convert the target into binary categories\n",
        "# Let's say the threshold is the median price\n",
        "threshold = np.median(y)\n",
        "y_binary = np.where(y > threshold, 1, 0)  # 1 for 'expensive', 0 for 'not expensive'\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "scaled_x_train = scaler.fit_transform(X_train)\n",
        "scaled_x_test = scaler.transform(X_test)\n",
        "# Apply logistic regression\n",
        "model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
        "model.fit(scaled_x_train, y_binary_train)\n",
        "\n",
        "# Now you can make predictions and evaluate the model\n",
        "predictions = model.predict(scaled_x_test)\n",
        "print(accuracy_score(y_binary_test,predictions))\n",
        "\n",
        "# Evaluating the model would typically involve metrics like accuracy, precision, and recall."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqzx-iVE4M5-",
        "outputId": "3cbf755c-5523-4ed8-c14e-89478b46e3c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8725490196078431\n"
          ]
        }
      ]
    }
  ]
}